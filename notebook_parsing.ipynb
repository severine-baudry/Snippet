{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from git import Repo\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_output_marking(notebook, output_mark, ignore_case = False):\n",
    "    '''\n",
    "    find all the notebook output cells with a text fields;\n",
    "    and return the text field only if it contains output_mark\n",
    "    returns : l_res[cell_id][text_idx] = text\n",
    "    '''\n",
    "    l_res=OrderedDict()\n",
    "    for cell_idx,cell in enumerate(notebook[\"cells\"]):\n",
    "        if cell[\"cell_type\"] == \"code\":\n",
    "            cell_id = cell[\"metadata\"][\"id\"]\n",
    "            idx=-1\n",
    "            res = OrderedDict()\n",
    "            for output in cell[\"outputs\"] :\n",
    "                if \"text\" in output :\n",
    "                    found = False\n",
    "                    idx += 1\n",
    "                    for line in output[\"text\"]:\n",
    "                        if ignore_case == True :\n",
    "                            if line.lower().find(output_mark.lower()) != -1 :\n",
    "                                found = True\n",
    "                        else :   \n",
    "                            if line.find(output_mark)!= -1:\n",
    "                                #print(\"MARK FOUND !\", cell_idx)\n",
    "                                found = True\n",
    "                    if found :\n",
    "                        res[idx] = output[\"text\"]\n",
    "            if len(res):\n",
    "                l_res[cell_id] = res\n",
    "    return l_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_output_cells(notebook, filter_fun):\n",
    "    '''\n",
    "    find in notebook all text ouput cells where filter_fun(cell) returns true\n",
    "    l_res[cell_idx][text_idx] = output_text\n",
    "    '''\n",
    "    l_res=OrderedDict()\n",
    "    for cell_idx,cell in enumerate(notebook[\"cells\"]):\n",
    "        if cell[\"cell_type\"] == \"code\":\n",
    "            #print(\"cell idx\", cell_idx)\n",
    "            if not \"id\" in cell[\"metadata\"] :\n",
    "                cell_id = \"#\"+str(cell_idx)\n",
    "            else :\n",
    "                cell_id = cell[\"metadata\"][\"id\"]\n",
    "            idx=-1\n",
    "            res = OrderedDict()\n",
    "            for output in cell[\"outputs\"] :\n",
    "                if \"text\" in output :\n",
    "                    idx +=1\n",
    "                    found = filter_fun(output[\"text\"])\n",
    "                    if found :\n",
    "                        res[idx] = output[\"text\"]\n",
    "                    \n",
    "            if len(res):\n",
    "                l_res[cell_id] = res\n",
    "    return l_res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_find_all_output_marking():\n",
    "    notebook = json.load( open(\"../P2_dog_classification/dog_app.ipynb\"))\n",
    "    dres = find_all_output_marking(notebook, \"Training Loss\")\n",
    "    for k,v in dres.items():\n",
    "        for i, lines in v.items():\n",
    "            print(k, i)\n",
    "            for line in lines :\n",
    "                print(\"  \" , line, end =\"\")\n",
    "test_find_all_output_marking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_output_marking_(notebook, output_mark):\n",
    "    l_res = find_output_marking(notebook, output_mark)\n",
    "    assert len(l_res) == 1\n",
    "    return l_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize_output(output_text):\n",
    "    '''\n",
    "    split notebook into tokens \n",
    "    separators : whitespace  ','  ':'\n",
    "    '''\n",
    "    res =[]\n",
    "    for line in output_text :\n",
    "    #    a = re.split(' |\\n|:', line)\n",
    "        #a = re.split('\\s|:|,', line)\n",
    "        # split outputs on whitespace, : and ,\n",
    "        split_line = re.split('[\\s|:|,]+', line)\n",
    "        # remove empty tokens\n",
    "        split_line = [ a for a in split_line if len(a)]\n",
    "        #print(split_line)\n",
    "        res.append(split_line)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def parse_nn_performances(split_lines):\n",
    "    '''\n",
    "    parse performances info from a notebook output result cell\n",
    "    assumes each line is a train OR valid OR test result\n",
    "    result line must follow the template =\n",
    "    [phase] Epoch <epoch> loss/accuracy <metric>\n",
    "    fields may be separated by : , or whitespace\n",
    "    '''\n",
    "    res = OrderedDict( )\n",
    "    for line in split_lines:\n",
    "        if 'VALID' in line :\n",
    "            res.setdefault('VALID', OrderedDict())\n",
    "            perf_type = 'VALID'\n",
    "        elif 'TEST' in line :\n",
    "            res.setdefault('TEST', OrderedDict())\n",
    "            perf_type = 'TEST'            \n",
    "        else :\n",
    "            perf_type = 'TRAIN'\n",
    "            res.setdefault('TRAIN', OrderedDict())\n",
    "        if not 'Epoch' in line :\n",
    "            print(\"not a result line\")\n",
    "            continue\n",
    "        d_index = {}\n",
    "        l_names = [ 'loss', 'accuracy', 'Epoch']\n",
    "        d_type = {'loss':float, 'accuracy':float, 'Epoch':int }\n",
    "        d_val = {}\n",
    "        for name in l_names :\n",
    "            d_index[name] = line.index(name)\n",
    "        if d_index['loss'] == -1 and d_index['accuracy'] == -1 :\n",
    "            print(\"error : not a performance line\", line)\n",
    "        else :\n",
    "            try :\n",
    "                for name, index in d_index.items():\n",
    "                    if index > -1 :\n",
    "                        d_val[name] = d_type[name](line[index+1])\n",
    "            except ValueError :\n",
    "                print(\"error conversion\", name, index, line)\n",
    "            else :\n",
    "                epoch = d_val[\"Epoch\"]\n",
    "                del(d_val['Epoch'])\n",
    "                for name, perf in d_val.items() :\n",
    "                    res[perf_type].setdefault(name, OrderedDict() )\n",
    "                    res[perf_type][name][ epoch] = perf\n",
    "    return res\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_performance(results, metric):\n",
    "    '''\n",
    "    plot the train and valid performance for the given metric (loss / accuracy)\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.plot( list( res[\"TRAIN\"][metric].keys()), list(res[\"TRAIN\"][metric].values() ) , label =\"TRAIN\")\n",
    "    plt.plot( list( res[\"VALID\"][metric].keys()), list(res[\"VALID\"][metric].values() ) , label =\"VALID\")\n",
    "    plt.legend()\n",
    "    plt.title(metric)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def apply_file_str(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        #print(type(args))\n",
    "        if type( args[0]) is str :\n",
    "            #print(\"str\")\n",
    "            with open(args[0]) as f :\n",
    "                #print(*args[1:], *kwargs)\n",
    "                return func(f, *args[1:], *kwargs)\n",
    "        else :\n",
    "            #print(\"stream\")\n",
    "            #print( *args, *kwargs)\n",
    "            return func(*args, *kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@apply_file_str\n",
    "def extract_perf(notebook_f, l_markers = [\"Begin Training\", \"TEST\" ] ):\n",
    "    '''\n",
    "    perse the notebook for markers in l_markers, indicating a result cell\n",
    "    split each line of the result cell\n",
    "    then parse each line to extract results\n",
    "    '''\n",
    "    notebook = json.load(notebook_f)\n",
    "    res = OrderedDict()\n",
    "    for output_mark in  l_markers :\n",
    "        # find output cell beginning with output_mark\n",
    "        output_text = find_output_marking(notebook, output_mark)\n",
    "        if output_text :\n",
    "            # tokenize output cell\n",
    "            split_lines = tokenize_output(output_text)\n",
    "            # extract results from cell\n",
    "            dict_result = parse_nn_performances(split_lines)\n",
    "            res.update(dict_result)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_notebook_train_valid(notebook_name, l_markers = [\"Begin Training\", \"TEST\" ]  ):\n",
    "    if type(notebook_name) is str :\n",
    "        with open(notebook_name) as f :\n",
    "            return extract_perf_(f, l_markers)\n",
    "    else :\n",
    "        return extract_perf_(notebook_name, l_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_notebook_parsing():\n",
    "    results = extract_perf(\"/home/severine/MOOCS/UDACITY/DEEP_LEARNING/TP/P2_dog_classification/Transfer_Learning_Solution_copy.ipynb\")\n",
    "    plot_performance(results, \"loss\")                 \n",
    "    plot_performance(results, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metric_phase(l_results, phase, metric):\n",
    "    '''\n",
    "    returns a DataFrame containing phase (train / valid / test) metric (loss / accuracy)\n",
    "    columns are commits sha\n",
    "    indexes are epochs\n",
    "    '''\n",
    "    df_result = pd.DataFrame()\n",
    "    df_commit_info = pd.DataFrame()\n",
    "    for result in l_results :\n",
    "        sha = result[\"sha\"][:5]\n",
    "        #print(result[\"res\"].keys())\n",
    "        try:\n",
    "            dres = result[\"res\"][phase][metric]\n",
    "        except KeyError :\n",
    "            pass\n",
    "        else :\n",
    "            df_current = pd.DataFrame.from_dict(dres, orient = 'index', columns =[sha] )\n",
    "            #print(df_current)\n",
    "            df_result = pd.concat([df_current, df_result], axis = 1)\n",
    "            \n",
    "            df_commit = pd.DataFrame( [[result[\"msg\"], result[\"date\"]] ] , columns = [\"message\", \"date\"], index =[sha])\n",
    "            df_commit_info = pd.concat([df_commit, df_commit_info], axis = 0)\n",
    "\n",
    "    return df_result, df_commit_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_commits_results(directory, notebook, parse_function = extract_perf): \n",
    "    '''\n",
    "    extract from all git versions of the notebook in directory the results\n",
    "    '''\n",
    "    repo = Repo(directory)\n",
    "    head = repo.head.reference\n",
    "    l_results = []\n",
    "    # iterate on the previous commits\n",
    "    for commit in list( repo.iter_commits( ) ) :\n",
    "        sha = commit.hexsha\n",
    "        msg = commit.message\n",
    "        dat = commit.authored_date\n",
    "        strdate = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(dat))\n",
    "        # dat = commit.commited_date\n",
    "        # files in the commit\n",
    "        for tr in commit.tree:\n",
    "            # load the notebook\n",
    "            if tr.name == notebook:\n",
    "                print(sha[:7], strdate, msg )\n",
    "                results = parse_function(tr.data_stream)\n",
    "                res_dict = OrderedDict( [(\"sha\", sha), (\"date\", strdate), (\"msg\", msg), (\"res\", results) ] )\n",
    "                l_results.append(res_dict)\n",
    "    return l_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit(directory, ** kwargs):\n",
    "    '''\n",
    "    get commit either with sha, or relative to a branch (HEAD by default)\n",
    "    '''\n",
    "    repo = Repo(directory)\n",
    "    branch_name = None\n",
    "    if \"sha\" in kwargs :\n",
    "        # find commit by sha\n",
    "        commit_ref = kwargs[\"sha\"]\n",
    "    else :\n",
    "        # find n last commit of branch\n",
    "        if \"branch\" in kwargs :\n",
    "            branch = kwargs[\"branch\"]\n",
    "            branch_name = branch\n",
    "        else :\n",
    "            branch = \"HEAD\"\n",
    "            branch_name = repo.active_branch.name\n",
    "        #print(branch_name, end = \"\\t\")\n",
    "        if \"num\" in kwargs :\n",
    "            branch += \"~\" + str(kwargs[\"num\"])\n",
    "        commit_ref = branch\n",
    "           \n",
    "    commit =  repo.commit(commit_ref)\n",
    "#    print(commit.hexsha[:7], commit.message)\n",
    "    return branch_name, commit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( get_commit(\"/home/severine/TEMP/P2_dog_classification\", num = 5))\n",
    "print( get_commit(\"/home/severine/TEMP/P2_dog_classification\"))\n",
    "print( get_commit(\"/home/severine/TEMP/P2_dog_classification\", branch = \"master\", num = 5))\n",
    "print( get_commit(\"/home/severine/TEMP/P2_dog_classification\", branch = \"master\"))\n",
    "print( get_commit(\"/home/severine/TEMP/P2_dog_classification\", sha = \"5dfbe8f65decc20e1d869b4cf265f9d6e33b1ae4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_element(commit, file_name):\n",
    "    for tr in commit.tree :\n",
    "        if tr.name == file_name :\n",
    "            return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gitdb.exc import (\n",
    "    BadObject,\n",
    "    BadName,\n",
    ")\n",
    "\n",
    "\n",
    "def get_notebook_results(directory, notebook_name, find_function, **kwargs):\n",
    "    try :\n",
    "        branch_name, commit = get_commit(directory, **kwargs)\n",
    "    except BadName as excpt:\n",
    "        print(\"ERROR : commit does not exist\")\n",
    "        print(excpt.args)\n",
    "        return None, None\n",
    "        \n",
    "    sha = commit.hexsha\n",
    "    msg = commit.message\n",
    "    dat = commit.authored_date\n",
    "    strdate = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(dat))\n",
    "    commit_info = OrderedDict( [ (\"sha\", sha), (\"msg\", msg),(\"date\", strdate)])\n",
    "    # get notebook for the current commit\n",
    "    notebook_blob = get_tree_element(commit, file_name=notebook_name)\n",
    "    notebook = json.load(notebook_blob.data_stream)\n",
    "    l_result_cells = filter_output_cells(notebook, find_function)\n",
    "    return l_result_cells, commit_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_any(text, l_marks, ignore_case = False):\n",
    "    '''\n",
    "    find in text \n",
    "    '''\n",
    "    found = False\n",
    "    for mark in l_marks : \n",
    "        if ignore_case :\n",
    "            m = mark.lower()\n",
    "            for line in text :\n",
    "                if line.lower().find(m) != -1 :\n",
    "                    return True\n",
    "        else :\n",
    "            for line in text :\n",
    "                if line.find(mark) != -1 :\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import parse\n",
    "def parse_train_out_1(line):\n",
    "    resparse = parse(\"Epoch: {epoch:d} \tTraining Loss: {train_loss:f} \tValidation Loss: {valid_loss:f}\ttrain correct :{train_accuracy:f}\tvalid correct :{valid_accuracy:f}\ttime {extim}\",\n",
    "                     line)\n",
    "    if resparse :\n",
    "        res = []\n",
    "        res.append(OrderedDict( [(\"epoch\", resparse.named[\"epoch\"]),\n",
    "                                 (\"phase\", \"train\"),\n",
    "                                 (\"metric\",\"loss\"),\n",
    "                                 (\"val\", resparse.named[\"train_loss\"]),\n",
    "                                 ]) )\n",
    "        res.append(OrderedDict( [(\"epoch\", resparse.named[\"epoch\"]),\n",
    "                                 (\"phase\", \"valid\"),\n",
    "                                 (\"metric\", \"loss\"),\n",
    "                                 (\"val\", resparse.named[\"valid_loss\"]),\n",
    "                                 ]) )\n",
    "        res.append(OrderedDict( [(\"epoch\", resparse.named[\"epoch\"]),\n",
    "                                 (\"phase\", \"train\"),\n",
    "                                 (\"metric\", \"accuracy\"),\n",
    "                                 (\"val\", resparse.named[\"train_accuracy\"]),\n",
    "                                 ]) )\n",
    "        res.append(OrderedDict( [(\"epoch\", resparse.named[\"epoch\"]),\n",
    "                                 (\"phase\", \"valid\"),\n",
    "                                 (\"metric\", \"accuracy\"),\n",
    "                                 (\"val\", resparse.named[\"valid_accuracy\"]),\n",
    "                                 ]) )\n",
    "    else :\n",
    "        res = None\n",
    "    target_nb = 6\n",
    "    return target_nb, res\n",
    "def parse_train_out_2(line):\n",
    "    resparse = parse(  \"Epoch: {epoch:d} \tTraining Loss: {train_loss:f} \tValidation Loss: {valid_loss:f}\tvalid correct :{valid_accuracy:f}\ttime {extim}\",\n",
    "                     line)\n",
    "    target_nb = 5\n",
    "    if resparse :\n",
    "        res = []\n",
    "        res.append(OrderedDict( [(\"epoch\", resparse.named[\"epoch\"]),\n",
    "                                 (\"phase\", \"train\"),\n",
    "                                 (\"metric\",\"loss\"),\n",
    "                                 (\"val\", resparse.named[\"train_loss\"]),\n",
    "                                 ]) )\n",
    "        res.append(OrderedDict( [(\"epoch\", resparse.named[\"epoch\"]),\n",
    "                                 (\"phase\", \"valid\"),\n",
    "                                 (\"metric\", \"loss\"),\n",
    "                                 (\"val\", resparse.named[\"valid_loss\"]),\n",
    "                                 ]) )\n",
    "        res.append(OrderedDict( [(\"epoch\", resparse.named[\"epoch\"]),\n",
    "                                 (\"phase\", \"valid\"),\n",
    "                                 (\"metric\", \"accuracy\"),\n",
    "                                 (\"val\", resparse.named[\"valid_accuracy\"]),\n",
    "                                 ]) )\n",
    "    else :\n",
    "        res = None\n",
    "    return target_nb, res\n",
    "\n",
    "def parse_testloss_1(line):\n",
    "    resparse = parse(\"Test Loss: {test_loss:f}\", line)\n",
    "    target_nb = 1\n",
    "    if resparse :\n",
    "        res = []\n",
    "        res.append(OrderedDict( [(\"phase\", \"test\"),\n",
    "                                 (\"metric\", \"loss\"),\n",
    "                                 (\"val\", resparse.named[\"test_loss\"])\n",
    "                                 ]) )\n",
    "    else :\n",
    "        res = None\n",
    "    return target_nb, res\n",
    "\n",
    "def parse_testacc_1(line):\n",
    "    target_nb =2 \n",
    "    resparse = parse(\"Test Accuracy: {test_accuracy}%{ratio}\", line)\n",
    "    if resparse :\n",
    "        res = []\n",
    "        res.append(OrderedDict( [(\"phase\", \"test\"),\n",
    "                                 (\"metric\", \"accuracy\"),\n",
    "                                 (\"val\", resparse.named[\"test_accuracy\"])\n",
    "                                 ]) )\n",
    "    else :\n",
    "        res = None\n",
    "    return target_nb, res\n",
    "    \n",
    "out1 = parse_train_out_1(\"Epoch: 6 \tTraining Loss: 4.887487 \tValidation Loss: 3.792227\ttrain correct :0.011\tvalid correct :0.011\ttime 00:03:40\")\n",
    "print(out1)\n",
    "parse_testacc_1(\"Test Accuracy: 12% (105/836)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteBookForCommit:\n",
    "    def get_commit(self, directory, notebook_name, **kwargs):\n",
    "        try :\n",
    "            branch_name, commit = get_commit(directory, **kwargs)\n",
    "        except BadName as excpt:\n",
    "            print(\"ERROR : commit does not exist\")\n",
    "            print(excpt.args)\n",
    "            self.sha = None\n",
    "            self.msg = None\n",
    "            self.strdate= None\n",
    "            self.notebook = None\n",
    "        else :\n",
    "            self.sha = commit.hexsha\n",
    "            self.msg = commit.message\n",
    "            dat = commit.authored_date\n",
    "            self.strdate = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(dat))\n",
    "            # get notebook for the current commit\n",
    "            notebook_blob = get_tree_element(commit, file_name=notebook_name)\n",
    "            self.notebook = json.load(notebook_blob.data_stream)\n",
    "    def filter_output_cells(self, find_function):\n",
    "        self.res_cells =filter_output_cells(self.notebook, find_function)\n",
    "        self.res_cell_keys = list(self.res_cells.keys())\n",
    "    def cell_source(self, cell_idx):\n",
    "        for cidx,cell in enumerate(notebook[\"cells\"]):\n",
    "            if cell[\"cell_type\"] == \"code\":\n",
    "                #print(\"cell idx\", cell_idx)\n",
    "                if not \"id\" in cell[\"metadata\"] :\n",
    "                    if  cell_idx == \"#\"+str(cidx):\n",
    "                        return cell[\"source\"]\n",
    "                else :\n",
    "                    if cell_idx == cell[\"metadata\"][\"id\"]:\n",
    "                        return cell[\"source\"]\n",
    "    def find_res_source(self):\n",
    "        self.res_cells_source = OrderedDict()\n",
    "        for cidx in self.res_cell_keys:\n",
    "            self.res_cells_source[cidx] = self.cell_source(cidx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "dogdir = \"/home/severine/TEMP/P2_dog_classification\"\n",
    "l_marks = [\"train\", \"valid\", \"test\", \"epoch\", \"loss\", \"accuracy\"]\n",
    "l_marks = [\"Test\"]\n",
    "l_marks=[\"loss\", \"accuracy\"]\n",
    "filter_fun = functools.partial(find_any, l_marks = l_marks, ignore_case = True)\n",
    "\n",
    "num = 0\n",
    "while 1 :\n",
    "    res_text, commit_info = get_notebook_results(dogdir, \"dog_app.ipynb\", find_function=filter_fun, num = num)\n",
    "    if commit_info == None :\n",
    "        break\n",
    "    print(num, len(res_text), list(res_text.keys()))\n",
    "    num += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(0,53):\n",
    "    res_text, commit_info = get_notebook_results(dogdir, \"dog_app.ipynb\", find_function=filter_fun, num = num)\n",
    "    lk=list(res_text.keys())\n",
    "    print(num, lk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_info = {}\n",
    "parse_info['ZFEGK0be3VJN'] =[\"scratch\", parse_train_out_1,parse_train_out_2]\n",
    "parse_info['LEzAt0RE3VJn'] =[\"scratch\", parse_testloss_1, parse_testacc_1 ]\n",
    "parse_info['Wlf9QUM63VKW'] =[\"transfer\", parse_train_out_1, parse_train_out_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_name, commit = get_commit(dogdir, sha = \"761c8e7\")\n",
    "notebook = json.load( get_tree_element(commit, \"dog_app.ipynb\").data_stream )\n",
    "for cell_idx,cell in enumerate(notebook[\"cells\"]):\n",
    "    if cell[\"cell_type\"] == \"code\":\n",
    "        if \"id\" in cell[\"metadata\"] and cell[\"metadata\"][\"id\"] in lk :\n",
    "            print(\"found\", cell[\"metadata\"][\"id\"] )\n",
    "            for line in cell[\"source\"]:\n",
    "                print(\"\\t\", line.rstrip())\n",
    "            print(\"---------------------\")\n",
    "            for output in cell[\"outputs\"]:\n",
    "                if \"text\" in output:\n",
    "                    for line in output[\"text\"] :\n",
    "                        print(\"\\t\", line.rstrip())\n",
    "                    print(\"-----------------\")\n",
    "            print(\"====================\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsha = [\"5d9b427\"]\n",
    "l_res_per_commit = []\n",
    "with open(\"parse_error.log\", \"w\") as ferr:\n",
    "    l_all_res= []\n",
    "    for num in range(54) :\n",
    "    #for sha in lsha :\n",
    "        notebook_commit = NoteBookForCommit()\n",
    "        notebook_commit.get_commit(dogdir,\"dog_app.ipynb\", num=num)\n",
    "        #notebook_commit.get_commit(dogdir,\"dog_app.ipynb\", sha=sha)\n",
    "        notebook_commit.filter_output_cells(filter_fun)\n",
    "        #print(notebook_commit.res_cell_keys)\n",
    "        shortsha = notebook_commit.sha[:7]\n",
    "        print(num, shortsha)\n",
    "        d_commit_info = OrderedDict()\n",
    "        d_commit_info[\"sha\"] = shortsha\n",
    "        d_commit_info[\"msg\"] = notebook_commit.msg\n",
    "        d_commit_info[\"date\"] = notebook_commit.strdate\n",
    "        \n",
    "        notebook_commit.find_res_source()\n",
    "        for k in notebook_commit.res_cell_keys :\n",
    "            source_lines = notebook_commit.res_cells_source[k]\n",
    "            source = \"\".join(source_lines)\n",
    "            if \"scratch\" in source :\n",
    "                if \"scratch\" != parse_info[k][0]:\n",
    "                    print(\"PB scratch!!\",parse_info[k][0] )\n",
    "                    ferr.write(\"{}\\t{}\\tscratch\\n\".format(shortsha,k ))\n",
    "            if \"transfer\" in source :\n",
    "                if \"transfer\" != parse_info[k][0]:\n",
    "                    print(\"PB transfer!!\",parse_info[k][0] )\n",
    "                    ferr.write(\"{}\\t{}\\ttransfer\\n\".format(shortsha,k ))\n",
    "            \n",
    "            results = notebook_commit.res_cells[k]\n",
    "            if len(results) == 0:\n",
    "                continue\n",
    "            if 0 not in results.keys():\n",
    "                for k,v in results.items() : \n",
    "                    print(k, v)\n",
    "                break\n",
    " #           print(\"\\t\", results.keys())\n",
    "            for line in results[0]:\n",
    "                if line.isspace() or len(line) ==0 :\n",
    "                    continue\n",
    "                for func in parse_info[k][1:]:\n",
    "                    target_nb, resline = func(line)\n",
    "                    if resline != None and target_nb == len(resline):\n",
    "                        break\n",
    "                \n",
    "                if resline == None or target_nb != len(resline):\n",
    "                    #print(\"ERROR parsing\",k, \":\", target_nb)\n",
    "                    #print(line.rstrip())\n",
    "                    ferr.write(\"{}\\t{}\\t{}\\n\".format(shortsha,k,line.rstrip() ))\n",
    "                if resline :\n",
    "                    for unit_res in resline :\n",
    "                        cur_res = d_commit_info.copy()\n",
    "                        cur_res[\"NN\"] = parse_info[k][0]\n",
    "                        cur_res.update(unit_res)\n",
    "                        l_all_res.append(cur_res)\n",
    "\n",
    "\n",
    "            #print(\"\".join(results[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict(zip(range(len(l_all_res)), l_all_res))\n",
    "print(len(l_all_res))\n",
    "b = pd.DataFrame.from_dict(a, orient = \"index\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat parse_error.log"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lres = all_commits_results(\"../P2_dog_classification/\", \"dog_app.ipynb\", parse_function = functools.partial(find_all_output_marking, output_mark = \"Training Loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    all_results = all_commits_results(\"../P2_dog_classification/\", \"Transfer_Learning_Solution_copy.ipynb\")\n",
    "    valid_loss, commit_info = extract_metric_phase(all_results, \"VALID\", \"loss\")\n",
    "    print(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    valid_loss.plot(title = \"valid loss\", figsize = (10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pd.set_option('display.max_colwidth', 0)\n",
    "    print(commit_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    valid_accuracy, commit_info = extract_metric_phase(all_results, \"VALID\", \"accuracy\")\n",
    "    plt.figure()\n",
    "    valid_accuracy.plot( title = \"valid accuracy\", figsize = (10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemple of execution**\n",
    "\n",
    "python notebook_parsing.py  -d ../P2_dog_classification/ -nb Transfer_Learning_Solution_copy.ipynb  -o dogs_transfer_learning.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-d\", dest = \"directory\", required = True)\n",
    "    parser.add_argument(\"-nb\", dest = \"notebook\", required = True)\n",
    "    parser.add_argument(\"-o\", dest = \"output\", required = True)\n",
    "    l_args = parser.parse_args()\n",
    "    \n",
    "    print(l_args.notebook)\n",
    "    print(l_args.output)\n",
    "    l_results = all_commits_results(l_args.directory, l_args.notebook)\n",
    "    with open(l_args.output, \"w\") as fs :\n",
    "        json.dump(l_results, fs, indent = 2)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "175px",
    "width": "253.8px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
